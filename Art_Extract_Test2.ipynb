{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hQ4SnhyfJQW4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import faiss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import zipfile\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from PIL import Image, UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C-7vcoYuJS58"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR = \"nga_data\"\n",
    "    METADATA_DIR = os.path.join(DATA_DIR, \"opendata-main\", \"data\")\n",
    "    IMAGES_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "   \n",
    "    data_url = \"https://github.com/NationalGalleryOfArt/opendata/archive/refs/heads/main.zip\"\n",
    "\n",
    "   \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = 224\n",
    "    EMBEDDING_SIZE = 512\n",
    "\n",
    "    \n",
    "    TOP_K = 10\n",
    "\n",
    "os.makedirs(Config.DATA_DIR, exist_ok=True)\n",
    "os.makedirs(Config.IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(Config.METADATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7X2hIdt3JS26",
    "outputId": "60de8a4e-5feb-4d8a-8652-a2f4a1e21e77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40.3kKB [00:05, 6.86kKB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting nga_data/main.zip to nga_data\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract(url, destination_folder):\n",
    "       \"\"\"Downloads and extracts a zip file from a given URL.\"\"\"\n",
    "\n",
    "       filename = os.path.basename(url)\n",
    "\n",
    "       download_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "       response = requests.get(url, stream=True)\n",
    "       total_size = int(response.headers.get('content-length', 0))\n",
    "       block_size = 1024\n",
    "\n",
    "       with open(download_path, 'wb') as f:\n",
    "           for data in tqdm(response.iter_content(block_size),\n",
    "                            total=total_size // block_size,\n",
    "                            unit='KB', unit_scale=True):\n",
    "               f.write(data)\n",
    "\n",
    "       print(f\"Extracting {download_path} to {destination_folder}\")\n",
    "       with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "           zip_ref.extractall(destination_folder)\n",
    "\n",
    "       os.remove(download_path)\n",
    "\n",
    "destination_folder = 'nga_data'\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "download_and_extract(Config.data_url, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0v0LRiAOJSz9"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'nga_data'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'opendata-main', 'data')\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, 'images')\n",
    "\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    objects_df = pd.read_csv(os.path.join(DATA_DIR, 'objects.csv'))\n",
    "    images_df = pd.read_csv(os.path.join(DATA_DIR, 'published_images.csv'))\n",
    "\n",
    "    paintings_df = objects_df[\n",
    "        ((objects_df['classification'].str.contains('painting', case=False, na=False)) |\n",
    "         (objects_df['classification'].str.contains('portrait', case=False, na=False)) |\n",
    "         (objects_df['title'].str.contains('portrait', case=False, na=False))) &\n",
    "        (objects_df['medium'].notna())]  \n",
    "\n",
    "    if 'style' in objects_df.columns:\n",
    "        paintings_df['style_category'] = paintings_df['style'].apply(\n",
    "            lambda x: categorize_style(x) if isinstance(x, str) else 'unknown'\n",
    "        )\n",
    "\n",
    "    painting_images = pd.merge(\n",
    "        paintings_df,\n",
    "        images_df,\n",
    "        left_on='objectid',\n",
    "        right_on='depictstmsobjectid',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    painting_images = painting_images[\n",
    "        (painting_images['viewtype'] == 'primary') |\n",
    "        (painting_images['viewtype'] == 'alternate') & (painting_images['iiifurl'].notna())\n",
    "    ]\n",
    "\n",
    "    painting_images['has_high_res'] = painting_images['width'] > 1000\n",
    "\n",
    "    return painting_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5NA_aEWiJSxA"
   },
   "outputs": [],
   "source": [
    "def categorize_style(style_text):\n",
    "    if not isinstance(style_text, str):\n",
    "        return 'unknown'\n",
    "\n",
    "    style_text = style_text.lower()\n",
    "    if any(term in style_text for term in ['renaissance', 'baroque', 'rococo']):\n",
    "        return 'classical'\n",
    "    elif any(term in style_text for term in ['impressionist', 'impressionism']):\n",
    "        return 'impressionism'\n",
    "    elif any(term in style_text for term in ['modern', 'contemporary', 'abstract']):\n",
    "        return 'modern'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def download_images(image_data, limit=10000):\n",
    "    \"\"\"\n",
    "    Download images from IIIF URLs with improved quality control\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    object_ids = []\n",
    "    metadata = []\n",
    "\n",
    "    existing_images = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.jpg')]\n",
    "    if len(existing_images) >= limit:\n",
    "        existing_ids = [f.split('.')[0] for f in existing_images]\n",
    "        sample_data = image_data[image_data['objectid'].astype(str).isin(existing_ids)]\n",
    "        sample_data = sample_data.head(limit)\n",
    "        print(f\"Using {len(sample_data)} existing images\")\n",
    "    else:\n",
    "        remaining = limit - len(existing_images)\n",
    "        existing_ids = [f.split('.')[0] for f in existing_images]\n",
    "        existing_data = image_data[image_data['objectid'].astype(str).isin(existing_ids)]\n",
    "        new_sample = image_data[~image_data['objectid'].astype(str).isin(existing_ids)]\n",
    "\n",
    "        if 'has_high_res' in new_sample.columns and len(new_sample[new_sample['has_high_res']]) > 0:\n",
    "            high_res_data = new_sample[new_sample['has_high_res']].sample(\n",
    "                min(remaining, len(new_sample[new_sample['has_high_res']]))\n",
    "            )\n",
    "            remaining_count = remaining - len(high_res_data)\n",
    "            if remaining_count > 0 and len(new_sample) > len(high_res_data):\n",
    "                remaining_data = new_sample[~new_sample['has_high_res']].sample(\n",
    "                    min(remaining_count, len(new_sample[~new_sample['has_high_res']]))\n",
    "                )\n",
    "                new_sample_data = pd.concat([high_res_data, remaining_data])\n",
    "            else:\n",
    "                new_sample_data = high_res_data\n",
    "        else:\n",
    "            new_sample_data = new_sample.sample(min(remaining, len(new_sample)))\n",
    "\n",
    "        sample_data = pd.concat([existing_data, new_sample_data])\n",
    "        print(f\"Using {len(existing_data)} existing images and downloading {len(new_sample_data)} new images\")\n",
    "\n",
    "    if 'has_high_res' in image_data.columns:\n",
    "        high_res_data = image_data[image_data['has_high_res']].sample(min(limit, len(image_data[image_data['has_high_res']])))\n",
    "        remaining_count = limit - len(high_res_data)\n",
    "        if remaining_count > 0 and len(image_data) > len(high_res_data):\n",
    "            remaining_data = image_data[~image_data['has_high_res']].sample(min(remaining_count, len(image_data[~image_data['has_high_res']])))\n",
    "            sample_data = pd.concat([high_res_data, remaining_data])\n",
    "        else:\n",
    "            sample_data = high_res_data\n",
    "    else:\n",
    "        sample_data = image_data.sample(min(limit, len(image_data)))\n",
    "\n",
    "    for _, row in tqdm(sample_data.iterrows(), total=len(sample_data)):\n",
    "        try:\n",
    "            img_url = f\"{row['iiifurl']}/full/1200,/0/default.jpg\"\n",
    "\n",
    "            file_path = os.path.join(IMAGE_DIR, f\"{row['objectid']}.jpg\")\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                response = requests.get(img_url)\n",
    "                if response.status_code == 200:\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "                    if img.width < 300 or img.height < 300:\n",
    "                        print(f\"Skipping low-quality image {row['objectid']}\")\n",
    "                        continue\n",
    "\n",
    "                    img.save(file_path)\n",
    "                    images.append(file_path)\n",
    "                    object_ids.append(row['objectid'])\n",
    "\n",
    "                    meta = {\n",
    "                        'objectid': row['objectid'],\n",
    "                        'title': row['title'] if 'title' in row else '',\n",
    "                        'artist': row['attribution'] if 'attribution' in row else '',\n",
    "                        'date': row['displaydate'] if 'displaydate' in row else '',\n",
    "                        'medium': row['medium'] if 'medium' in row else '',\n",
    "                        'style': row.get('style_category', 'unknown')\n",
    "                    }\n",
    "                    metadata.append(meta)\n",
    "            else:\n",
    "                img = Image.open(file_path)\n",
    "                if img.width >= 300 and img.height >= 300:\n",
    "                    images.append(file_path)\n",
    "                    object_ids.append(row['objectid'])\n",
    "\n",
    "                    meta = {\n",
    "                        'objectid': row['objectid'],\n",
    "                        'title': row['title'] if 'title' in row else '',\n",
    "                        'artist': row['attribution'] if 'attribution' in row else '',\n",
    "                        'date': row['displaydate'] if 'displaydate' in row else '',\n",
    "                        'medium': row['medium'] if 'medium' in row else '',\n",
    "                        'style': row.get('style_category', 'unknown')\n",
    "                    }\n",
    "                    metadata.append(meta)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {row['objectid']}: {e}\")\n",
    "\n",
    "    return images, object_ids, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Msy_pMfvWEnJ"
   },
   "outputs": [],
   "source": [
    "def get_preprocess_transform(train=True):\n",
    "    \"\"\"\n",
    "    Returns a preprocessing transform with data augmentation for training\n",
    "    and standard preprocessing for validation/testing.\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),  \n",
    "            transforms.RandomHorizontalFlip(), \n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  \n",
    "            transforms.RandomRotation(20), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wb69E6KjJSto"
   },
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def extract_features(image_paths, train=True, num_epochs=2):\n",
    "    \"\"\"\n",
    "    Extract features using multiple models for better representation.\n",
    "    If train=True, fine-tune the models for the given number of epochs.\n",
    "    \"\"\"\n",
    "    models_list = {\n",
    "        'resnet101': models.resnet101(pretrained=True),\n",
    "        'efficientnet': models.efficientnet_b3(pretrained=True),\n",
    "        'vit': models.vit_b_16(pretrained=True)\n",
    "    }\n",
    "\n",
    "    for name, model in models_list.items():\n",
    "        if name == 'vit':\n",
    "            models_list[name].heads = torch.nn.Identity()\n",
    "        else:\n",
    "            models_list[name] = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        models_list[name].eval()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for name in models_list:\n",
    "        models_list[name] = models_list[name].to(device)\n",
    "\n",
    "    preprocess = get_preprocess_transform(train=train)\n",
    "\n",
    "    all_features = []\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    if train:\n",
    "        feature_size = 4352  \n",
    "        num_classes = 2  \n",
    "        classifier = nn.Linear(feature_size, num_classes).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            for img_path in tqdm(image_paths):\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    boxes, _ = mtcnn.detect(img)\n",
    "                    if boxes is not None:\n",
    "                        x_min, y_min = np.min(boxes[:, 0]), np.min(boxes[:, 1])\n",
    "                        x_max, y_max = np.max(boxes[:, 2]), np.max(boxes[:, 3])\n",
    "                        img = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "                    img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "                    features_combined = []\n",
    "                    with torch.no_grad():\n",
    "                        for name, model in models_list.items():\n",
    "                            feature = model(img_tensor)\n",
    "                            if len(feature.shape) > 2:\n",
    "                                feature = feature.reshape(feature.size(0), -1)\n",
    "                            feature = feature.cpu().numpy()\n",
    "                            features_combined.append(feature.squeeze())\n",
    "\n",
    "                    combined = np.concatenate(features_combined)\n",
    "                    combined = scaler.fit_transform(combined.reshape(-1, 1)).flatten()\n",
    "                    all_features.append(combined)\n",
    "\n",
    "                    combined_tensor = torch.tensor(combined).unsqueeze(0).to(device)\n",
    "\n",
    "                    outputs = classifier(combined_tensor)\n",
    "                    labels = torch.tensor([0]).to(device)  # Replace with actual labels\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "                    if all_features:\n",
    "                        zero_dim = all_features[0].shape[0]\n",
    "                        all_features.append(np.zeros(zero_dim))\n",
    "                    else:\n",
    "                        print(\"Failed to process first image, cannot determine feature dimension\")\n",
    "                        raise\n",
    "\n",
    "    else:\n",
    "        for img_path in tqdm(image_paths):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                boxes, _ = mtcnn.detect(img)\n",
    "                if boxes is not None:\n",
    "                    x_min, y_min = np.min(boxes[:, 0]), np.min(boxes[:, 1])\n",
    "                    x_max, y_max = np.max(boxes[:, 2]), np.max(boxes[:, 3])\n",
    "                    img = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "                img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "                features_combined = []\n",
    "                with torch.no_grad():\n",
    "                    for name, model in models_list.items():\n",
    "                        feature = model(img_tensor)\n",
    "                        if len(feature.shape) > 2:\n",
    "                            feature = feature.reshape(feature.size(0), -1)\n",
    "                        feature = feature.cpu().numpy()\n",
    "                        features_combined.append(feature.squeeze())\n",
    "\n",
    "                combined = np.concatenate(features_combined)\n",
    "                combined = scaler.fit_transform(combined.reshape(-1, 1)).flatten()\n",
    "                all_features.append(combined)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                if all_features:\n",
    "                    zero_dim = all_features[0].shape[0]\n",
    "                    all_features.append(np.zeros(zero_dim))\n",
    "                else:\n",
    "                    print(\"Failed to process first image, cannot determine feature dimension\")\n",
    "                    raise\n",
    "\n",
    "    features_array = np.array(all_features)\n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5vVWMbosJSqg"
   },
   "outputs": [],
   "source": [
    "def optimize_features(features, n_components=512):\n",
    "    \"\"\"\n",
    "    Apply PCA and normalization to optimize features\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "    print(f\"Explained variance ratio sum: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "    return features_pca, scaler, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xk9l0MXBJSkg"
   },
   "outputs": [],
   "source": [
    "def build_similarity_index(features, metadata=None):\n",
    "    \"\"\"\n",
    "    Build a more advanced FAISS index with metadata support\n",
    "    \"\"\"\n",
    "    features = features.astype('float32')\n",
    "    faiss.normalize_L2(features)\n",
    "\n",
    "    d = features.shape[1] \n",
    "    nlist = min(100, features.shape[0] // 10)  \n",
    "\n",
    "    kmeans = faiss.Kmeans(d, nlist, niter=20, verbose=True)\n",
    "    kmeans.train(features)\n",
    "\n",
    "    quantizer = faiss.IndexFlatIP(d)\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "    index.train(features)\n",
    "    index.add(features)\n",
    "\n",
    "    index_with_meta = {\n",
    "        'index': index,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "    return index_with_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3S8HrccCJSe7"
   },
   "outputs": [],
   "source": [
    "def find_similar_images(index_with_meta, features, query_idx, metadata=None, k=5):\n",
    "    \"\"\"\n",
    "    Find similar images with metadata-enhanced similarity using cosine similarity.\n",
    "    \"\"\"\n",
    "    index = index_with_meta['index']\n",
    "    meta_list = index_with_meta['metadata']\n",
    "\n",
    "    query_vector = features[query_idx].reshape(1, -1).astype('float32')\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    k_search = min(k * 3, features.shape[0])\n",
    "    D, I = index.search(query_vector, k_search)\n",
    "\n",
    "    candidate_vectors = features[I[0]]\n",
    "    cos_sim = cosine_similarity(query_vector, candidate_vectors)[0]\n",
    "\n",
    "    if meta_list and metadata:\n",
    "        query_meta = meta_list[query_idx]\n",
    "\n",
    "        combined_scores = []\n",
    "        for i, idx in enumerate(I[0]):\n",
    "            if idx < len(meta_list):\n",
    "                candidate_meta = meta_list[idx]\n",
    "                \n",
    "                meta_sim = calculate_metadata_similarity(query_meta, candidate_meta)\n",
    "                \n",
    "                combined_score = 0.7 * cos_sim[i] + 0.3 * meta_sim\n",
    "                combined_scores.append((idx, combined_score))\n",
    "\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_indices = [idx for idx, _ in combined_scores[:k]]\n",
    "        top_scores = [score for _, score in combined_scores[:k]]\n",
    "    else:\n",
    "        top_indices = np.argsort(-cos_sim)[:k]\n",
    "        top_scores = cos_sim[top_indices]\n",
    "\n",
    "    return top_scores, top_indices\n",
    "\n",
    "def calculate_metadata_similarity(meta1, meta2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two metadata entries\n",
    "    \"\"\"\n",
    "    similarity = 0.0\n",
    "\n",
    "    if 'style' in meta1 and 'style' in meta2 and meta1['style'] == meta2['style']:\n",
    "        similarity += 0.4\n",
    "\n",
    "    if 'artist' in meta1 and 'artist' in meta2 and meta1['artist'] == meta2['artist']:\n",
    "        similarity += 0.3\n",
    "\n",
    "    if 'medium' in meta1 and 'medium' in meta2:\n",
    "        if meta1['medium'] == meta2['medium']:\n",
    "            similarity += 0.2\n",
    "        elif isinstance(meta1['medium'], str) and isinstance(meta2['medium'], str):\n",
    "            # Partial match\n",
    "            if any(term in meta2['medium'].lower() for term in meta1['medium'].lower().split()):\n",
    "                similarity += 0.1\n",
    "\n",
    "    if 'date' in meta1 and 'date' in meta2:\n",
    "        meta1_period = extract_period(meta1['date'])\n",
    "        meta2_period = extract_period(meta2['date'])\n",
    "        if meta1_period and meta2_period and abs(meta1_period - meta2_period) < 50:\n",
    "            similarity += 0.1\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def extract_period(date_str):\n",
    "    \"\"\"\n",
    "    Extract a numeric year from a date string\n",
    "    \"\"\"\n",
    "    if not isinstance(date_str, str):\n",
    "        return None\n",
    "\n",
    "    import re\n",
    "    years = re.findall(r'\\b(1[4-9]\\d\\d|20[0-2]\\d)\\b', date_str)\n",
    "    if years:\n",
    "        return int(years[0])\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U8TvS9Z6JSb5"
   },
   "outputs": [],
   "source": [
    "def visualize_similar_images(query_idx, similar_indices, image_paths, object_ids, metadata=None):\n",
    "    \"\"\"\n",
    "    Visualize the query image and its similar images with metadata\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    query_img = Image.open(image_paths[query_idx])\n",
    "    plt.imshow(query_img)\n",
    "    title = f\"Query: {object_ids[query_idx]}\"\n",
    "    if metadata and query_idx < len(metadata):\n",
    "        meta = metadata[query_idx]\n",
    "        if 'title' in meta and meta['title']:\n",
    "            title += f\"\\n{meta['title']}\"\n",
    "        if 'artist' in meta and meta['artist']:\n",
    "            title += f\"\\nArtist: {meta['artist']}\"\n",
    "        if 'style' in meta and meta['style']:\n",
    "            title += f\"\\nStyle: {meta['style']}\"\n",
    "\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i, idx in enumerate(similar_indices):\n",
    "        plt.subplot(2, 3, i+2)\n",
    "        similar_img = Image.open(image_paths[idx])\n",
    "        plt.imshow(similar_img)\n",
    "\n",
    "        title = f\"Similar: {object_ids[idx]}\"\n",
    "        if metadata and idx < len(metadata):\n",
    "            meta = metadata[idx]\n",
    "            if 'title' in meta and meta['title']:\n",
    "                title += f\"\\n{meta['title']}\"\n",
    "            if 'artist' in meta and meta['artist']:\n",
    "                title += f\"\\nArtist: {meta['artist']}\"\n",
    "            if 'style' in meta and meta['style']:\n",
    "                title += f\"\\nStyle: {meta['style']}\"\n",
    "\n",
    "        plt.title(title, fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'similar_images.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ow-Q1XTrJSYj"
   },
   "outputs": [],
   "source": [
    "def convert_np_types(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist() \n",
    "    elif isinstance(obj, np.float32):\n",
    "        return float(obj)  \n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_np_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_np_types(item) for item in obj]\n",
    "    return obj \n",
    "\n",
    "def evaluate_model(index_with_meta, features, metadata=None, k=5, save_dir='results'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    precision_at_k = []\n",
    "    recall_at_k = []\n",
    "    average_precision = []\n",
    "    rmse_scores = []\n",
    "    ssim_scores = []\n",
    "\n",
    "    if metadata:\n",
    "        clusters = {}\n",
    "        for i, meta in enumerate(metadata):\n",
    "            key = meta.get('artist') or meta.get('style') or meta.get('medium') or 'unknown'\n",
    "            if key not in clusters:\n",
    "                clusters[key] = []\n",
    "            clusters[key].append(i)\n",
    "\n",
    "        valid_clusters = {key: indices for key, indices in clusters.items() if len(indices) >= 2}\n",
    "        \n",
    "        import random\n",
    "        eval_indices = []\n",
    "        for key, indices in valid_clusters.items():\n",
    "            if len(indices) > 5:\n",
    "                eval_indices.extend(random.sample(indices, min(5, len(indices))))\n",
    "        if not eval_indices:\n",
    "            eval_indices = random.sample(range(len(features)), min(100, len(features)))\n",
    "    else:\n",
    "        import random\n",
    "        eval_indices = random.sample(range(len(features)), min(100, len(features)))\n",
    "\n",
    "    for i in eval_indices:\n",
    "        D, I = find_similar_images(index_with_meta, features, i, metadata, k)\n",
    "\n",
    "        if metadata and i in [idx for cluster in valid_clusters.values() for idx in cluster]:\n",
    "            relevant_cluster = next((indices for indices in valid_clusters.values() if i in indices), None)\n",
    "            relevant = set(relevant_cluster) if relevant_cluster else set()\n",
    "            relevant.discard(i)\n",
    "        else:\n",
    "            relevant = set(range(max(0, i - k // 2), min(len(features), i + k // 2 + 1)))\n",
    "            relevant.discard(i)\n",
    "\n",
    "        retrieved = set(I)\n",
    "\n",
    "        precision_at_k.append(len(relevant.intersection(retrieved)) / len(retrieved) if retrieved else 0)\n",
    "\n",
    "        recall_at_k.append(len(relevant.intersection(retrieved)) / len(relevant) if relevant else 0)\n",
    "\n",
    "        ap = 0\n",
    "        relevant_count = 0\n",
    "        for j in range(len(I)):\n",
    "            if I[j] in relevant:\n",
    "                relevant_count += 1\n",
    "                ap += relevant_count / (j + 1)\n",
    "        average_precision.append(ap / relevant_count if relevant_count else 0)\n",
    "\n",
    "        query_image = features[i].reshape(-1)  # Flatten the query image for RMSE\n",
    "        rmse_vals = []\n",
    "        ssim_vals = []\n",
    "\n",
    "        for idx in I:\n",
    "            retrieved_image = features[idx].reshape(-1)\n",
    "            rmse_vals.append(mean_squared_error(query_image, retrieved_image, squared=False))  # RMSE\n",
    "            ssim_vals.append(ssim(features[i], features[idx], data_range=features[idx].max() - features[idx].min()))  # SSIM\n",
    "\n",
    "        rmse_scores.append(np.mean(rmse_vals))\n",
    "        ssim_scores.append(np.mean(ssim_vals))\n",
    "\n",
    "    mean_precision_at_k = np.mean(precision_at_k)\n",
    "    mean_recall_at_k = np.mean(recall_at_k)\n",
    "    mean_average_precision = np.mean(average_precision)\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    mean_ssim = np.mean(ssim_scores)\n",
    "\n",
    "    print(f\"Precision@{k}: {mean_precision_at_k:.4f}\")\n",
    "    print(f\"Recall@{k}: {mean_recall_at_k:.4f}\")\n",
    "    print(f\"Mean Average Precision: {mean_average_precision:.4f}\")\n",
    "    print(f\"Mean RMSE: {mean_rmse:.4f}\")\n",
    "    print(f\"Mean SSIM: {mean_ssim:.4f}\")\n",
    "\n",
    "    results = {\n",
    "        'precision_at_k': mean_precision_at_k,\n",
    "        'recall_at_k': mean_recall_at_k,\n",
    "        'mean_average_precision': mean_average_precision,\n",
    "        'mean_rmse': mean_rmse,\n",
    "        'mean_ssim': mean_ssim\n",
    "    }\n",
    "\n",
    "    results_converted = convert_np_types(results)\n",
    "\n",
    "    with open(os.path.join(save_dir, 'evaluation_results.json'), 'w') as f:\n",
    "        json.dump(results_converted, f, indent=4)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(precision_at_k)), precision_at_k, label='Precision@K', marker='o')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Precision@K')\n",
    "    plt.title(f'Precision@{k}')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'precision_at_{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(recall_at_k)), recall_at_k, label='Recall@K', marker='o', color='orange')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Recall@K')\n",
    "    plt.title(f'Recall@{k}')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'recall_at_{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(average_precision)), average_precision, label='Average Precision', marker='o', color='green')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.title(f'Mean Average Precision (mAP)')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'map_{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(rmse_scores)), rmse_scores, label='RMSE', marker='o', color='red')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Root Mean Square Error (RMSE)')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'rmse.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(ssim_scores)), ssim_scores, label='SSIM', marker='o', color='purple')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.title('Structural Similarity Index (SSIM)')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'ssim.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return mean_precision_at_k, mean_recall_at_k, mean_average_precision, mean_rmse, mean_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MBp6jCYiJSVc"
   },
   "outputs": [],
   "source": [
    "face_detector = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def find_similar_images_for_user_input(index_with_meta, features, image_paths, object_ids, metadata, user_image_path, pca=None, scaler=None, face_crop=False):\n",
    "    \"\"\"\n",
    "    Find similar images for a user-provided image with similarity assessment.\n",
    "    \"\"\"\n",
    "    models_list = {\n",
    "        'resnet101': models.resnet101(pretrained=True),\n",
    "        'efficientnet': models.efficientnet_b3(pretrained=True),\n",
    "        'vit': models.vit_b_16(pretrained=True)\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for name, model in models_list.items():\n",
    "        if name == 'vit':\n",
    "            models_list[name].head = torch.nn.Sequential()\n",
    "        else:\n",
    "            models_list[name] = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        models_list[name].eval().to(device)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        img = Image.open(user_image_path).convert('RGB')\n",
    "        if face_crop:\n",
    "            boxes, _ = face_detector.detect(img)\n",
    "            if boxes is not None:\n",
    "                x, y, x2, y2 = boxes[0]\n",
    "                img = img.crop((x, y, x2, y2))\n",
    "\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "        features_combined = []\n",
    "        with torch.no_grad():\n",
    "            for name, model in models_list.items():\n",
    "                feature = model(img_tensor)\n",
    "                if len(feature.shape) > 2:\n",
    "                    feature = feature.view(feature.size(0), -1)\n",
    "                features_combined.append(feature.cpu().numpy().squeeze())\n",
    "\n",
    "        user_feature = np.concatenate(features_combined).reshape(1, -1)\n",
    "\n",
    "        if scaler and pca:\n",
    "            user_feature = scaler.transform(user_feature)\n",
    "            user_feature = pca.transform(user_feature)\n",
    "\n",
    "        faiss.normalize_L2(user_feature)\n",
    "\n",
    "        index = index_with_meta['index']\n",
    "        D, I = index.search(user_feature, 5)  # K = 5 (top 5 neighbors)\n",
    "\n",
    "        user_meta = {\n",
    "            'objectid': 'User',\n",
    "            'title': 'User Image',\n",
    "            'artist': '',\n",
    "            'date': '',\n",
    "            'medium': '',\n",
    "            'style': ''\n",
    "        }\n",
    "\n",
    "        paths_to_show = [user_image_path]\n",
    "        ids_to_show = ['User Image']\n",
    "        meta_to_show = [user_meta]\n",
    "\n",
    "        for idx in I[0]:\n",
    "            paths_to_show.append(image_paths[idx])\n",
    "            ids_to_show.append(object_ids[idx])\n",
    "            if metadata and idx < len(metadata):\n",
    "                meta_to_show.append(metadata[idx])\n",
    "            else:\n",
    "                meta_to_show.append(None)\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"User Query Image\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "        for i, idx in enumerate(I[0]):\n",
    "            plt.subplot(2, 3, i + 2)\n",
    "            similar_img = Image.open(image_paths[idx])\n",
    "            plt.imshow(similar_img)\n",
    "\n",
    "            title = f\"Similar: {object_ids[idx]}\"\n",
    "            if metadata and idx < len(metadata):\n",
    "                meta = metadata[idx]\n",
    "                if 'title' in meta and meta['title']:\n",
    "                    title += f\"\\n{meta['title']}\"\n",
    "                if 'artist' in meta and meta['artist']:\n",
    "                    title += f\"\\nArtist: {meta['artist']}\"\n",
    "                if 'style' in meta and meta['style']:\n",
    "                    title += f\"\\nStyle: {meta['style']}\"\n",
    "\n",
    "            plt.title(title, fontsize=10)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(BASE_DIR, 'user_query_results.png'))\n",
    "        plt.show()\n",
    "\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"Error: Cannot open image file {user_image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user image: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5MFAszp8QTH",
    "outputId": "e7ea635b-fa09-468a-85f7-af40eca77f6a"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    painting_images = load_data()\n",
    "    print(f\"Found {len(painting_images)} paintings with images\")\n",
    "\n",
    "    print(\"Downloading images...\")\n",
    "    image_paths, object_ids, metadata = download_images(painting_images, limit=10000)\n",
    "    print(f\"Downloaded {len(image_paths)} images\")\n",
    "\n",
    "    print(\"Extracting features using multiple models...\")\n",
    "    features = extract_features(image_paths)\n",
    "    print(f\"Extracted raw features with shape {features.shape}\")\n",
    "\n",
    "    print(\"Optimizing features...\")\n",
    "    features_optimized, scaler, pca = optimize_features(features, n_components=512)\n",
    "    print(f\"Optimized features to shape {features_optimized.shape}\")\n",
    "\n",
    "    print(\"Building advanced similarity index...\")\n",
    "    index_with_meta = build_similarity_index(features_optimized, metadata)\n",
    "\n",
    "    print(\"Evaluating improved model...\")\n",
    "    precision, recall, map_score, rmse, ssim = evaluate_model(index_with_meta, features_optimized, metadata, k=5)\n",
    "\n",
    "    query_idx = np.random.randint(0, len(image_paths))\n",
    "    print(f\"Finding similar images for query index {query_idx}...\")\n",
    "    D, I = find_similar_images(index_with_meta, features_optimized, query_idx, metadata, k=5)\n",
    "\n",
    "    print(\"Visualizing results...\")\n",
    "    visualize_similar_images(query_idx, I, image_paths, object_ids, metadata)\n",
    "\n",
    "    user_image_path=\"r_image.jpg\"  # Replace with the actual path\n",
    "    if os.path.exists(user_image_path):\n",
    "        print(f\"Finding similar images for user-provided image: {user_image_path}\")\n",
    "        find_similar_images_for_user_input(\n",
    "            index_with_meta, features_optimized, image_paths,\n",
    "            object_ids, metadata, user_image_path, pca, scaler\n",
    "        )\n",
    "    else:\n",
    "        print(\"User image not found. Please provide a valid path.\")\n",
    "\n",
    "    return painting_images, image_paths, object_ids, features_optimized, index_with_meta, metadata\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
